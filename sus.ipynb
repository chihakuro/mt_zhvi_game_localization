{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of XLSX file paths (each file represents a separate dataset)\n",
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "csv_files = ['flang.csv', 'train.csv', 'test.csv']\n",
    "# Proportion for training data (e.g., 80%)\n",
    "train_proportion = 0.8\n",
    "\n",
    "# Initialize lists to store training and testing dataframes\n",
    "train_dataframes = []\n",
    "test_dataframes = []\n",
    "\n",
    "# Loop through each XLSX file\n",
    "for csv_file in csv_files:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, train_size=train_proportion)\n",
    "\n",
    "    # Append the training and testing DataFrames to the lists\n",
    "    train_dataframes.append(train_df)\n",
    "    test_dataframes.append(test_df)\n",
    "\n",
    "# Stack the training and testing DataFrames into one DataFrame with 2 columns\n",
    "train_df = pd.concat(train_dataframes, axis=0)\n",
    "test_df = pd.concat(test_dataframes, axis=0)\n",
    "\n",
    "# Save the training and testing DataFrames to CSV files\n",
    "train_df.to_csv(os.path.join(parent_dir, 'trainf.csv'), index=False, header=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(os.path.join(parent_dir, 'testf.csv'), index=False, header=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "fil = 'flang.xlsx'\n",
    "df = pd.read_excel(parent_dir + fil)\n",
    "df.to_csv(parent_dir + 'dataset.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = 'trainf.csv'\n",
    "df = pd.read_csv(parent_dir + fil, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'testf.csv', header=None, names=['text', 'ground_truth'])\n",
    "def html_to_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(html_to_text)\n",
    "# change type of ground_truth to string\n",
    "df['ground_truth'] = df['ground_truth'].astype(str)\n",
    "df['ground_truth'] = df['ground_truth'].apply(html_to_text)\n",
    "\n",
    "# save to csv (without)\n",
    "df.to_csv(parent_dir + 'test.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'dataseto.csv', header=None, names=['text', 'ground_truth'])\n",
    "\n",
    "# remove new line characters in each column\n",
    "df['text'] = df['text'].replace('\\n', ' ')\n",
    "df['ground_truth'] = df['ground_truth'].replace('\\n', ' ')\n",
    "\n",
    "# save to csv (without)\n",
    "df.to_csv(parent_dir + 'dataset.csv', index=False, header=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print a sample\n",
    "#extract content from dataframe\n",
    "one = df.iloc[6]\n",
    "print(one['text'])\n",
    "print(new_line_to_space('种\\n植'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'testf.csv', header=None, names=['text', 'ground_truth'])\n",
    "\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = df[df['text']== df['ground_truth']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(df,shit, indicator=True, how = 'outer').query('_merge==\"left_only\"').drop('_merge',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "57056-52974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv(parent_dir + 'testf.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a text file and convert all to lowercase\n",
    "with open(parent_dir + 'raw_vietnamese.txt', 'w') as s:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "inp = parent_dir + 'raw_vietnamese.txt'\n",
    "model_prefix = 'spm_model'\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=inp, model_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = parent_dir + \"spm_model.model\"\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "\n",
    "text = \"Tính năng kết hôn vô cùng phong phú bao gồm bồi dưỡng tình cảm, phụ bản tình duyên, tặng hoa, khóa đồng tâm, bảo bảo…giúp cho bạn và nửa kia ngày càng thắt chặt tình cảm - ”Đấu Thần Tuyệt Thế - Không lo bị ế!\"\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "\n",
    "# Decode tokens back to text\n",
    "decoded_text = sp.decode_pieces(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Chinese text\n",
    "input_text = \"你好，这是一个机器翻译的示例。\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids, num_beams=4, max_length=150, no_repeat_ngram_size=3)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import MarianMTModel, MarianTokenizer\n",
    "# Start from here when restarting the notebook\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "import torch\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-vi\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Input: 想要用暗箭偷袭我？明智的选择，可惜失败了。\n",
      "Vietnamese Translation: Một sự lựa chọn khôn ngoan khi tấn công tôi bằng mũi tên tối tăm, nhưng không may là chúng ta đã thất bại.\n"
     ]
    }
   ],
   "source": [
    "# Input Chinese text\n",
    "input_text = \"想要用暗箭偷袭我？明智的选择，可惜失败了。\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', 'target'],\n",
       "    num_rows: 70953\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train MarianMT on custom dataset \n",
    "'''from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "\n",
    "df = pd.read_csv(parent_dir + 'dataset.csv', header=None, names=['text', 'ground_truth'], encoding='utf-8-sig')\n",
    "\n",
    "translation = df['text'].astype(str).tolist() #key\n",
    "target = df['ground_truth'].astype(str).tolist() #value\n",
    "\n",
    "# Initialize to a list of dictionaries in the format of {'translation': [translation1, translation2, ...], 'target': [target1, target2, ...]}\n",
    "data = {'translation': translation, 'target': target}\n",
    "\n",
    "# Convert the dictionary into a Dataset object\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets inside the Dataset object\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_val_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "train_val_dataset = train_val_dataset.train_test_split(test_size=0.25)\n",
    "train_dataset = train_val_dataset['train']\n",
    "val_dataset = train_val_dataset['test']\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>活动尚未开启，敬请期待</td>\n",
       "      <td>Hoạt động chưa mở, vui lòng đợi!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>分 享</td>\n",
       "      <td>Chia sẻ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>古道热肠</td>\n",
       "      <td>Nhiệt tình</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3星12阶红色仙镯</td>\n",
       "      <td>Tiên Trạc Đỏ bậc 12 3 Sao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>战车直升10阶礼包</td>\n",
       "      <td>Quà Chiến Xa lên giai 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e69ad690302b>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': '焰岭≥3个,圣辉≤1个', 'target': 'Hệ Hỏa ≥ 3, Hệ Kim ≤ 1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1198, 5292, 22, 809, 1943, 10313, 5228, 2429, 3, 693, 2791, 1117, 2455, 1176, 1943, 8401, 5228, 3, 2837, 1713, 911, 175, 1176, 1943, 8, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer(['您获得了真龙秘宝令，可进入这里进行金龙寻宝，抽取无上金龙！'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[5, 1, 22, 1, 3, 1, 3, 1, 8, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haimi\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"您获得了真龙秘宝令，可进入这里进行金龙寻宝，抽取无上金龙！\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 24827, 26521, 1, 656, 546, 3, 3073, 18478, 1, 1090, 546, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], [5, 2674, 519, 3, 620, 3341, 6899, 13696, 656, 546, 3341, 5952, 6037, 5971, 34241, 1090, 6213, 3902, 3, 1923, 3341, 720, 1516, 2763, 36287, 519, 3, 7211, 2797, 992, 1090, 36549, 15, 3902, 3, 3154, 2063, 3112, 8632, 1090, 2059, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[7518, 8558, 5, 1, 625, 3, 7518, 3882, 5, 1, 110, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], [1100, 149, 152, 625, 2731, 2672, 1123, 1222, 839, 956, 167, 918, 48, 364, 44718, 34241, 1090, 6213, 295, 459, 2, 4853, 839, 9, 1207, 2763, 1552, 1552, 3, 526, 30592, 5971, 295, 459, 2, 2207, 924, 8509, 1550, 2671, 110, 249, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "source_lang = \"translation\"\n",
    "target_lang = \"target\"\n",
    "data_list = [train_dataset, val_dataset, test_dataset]\n",
    "def preprocess_function(data_list):\n",
    "    inputs = data_list[\"translation\"]\n",
    "    targets = data_list[\"target\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "    # set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_function(train_dataset[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4319ef6e03204770aedc5a7a81860dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc2ddb4670a4cdfbccee70a2be1c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c34b49493b4802b07be5b29c2b33e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training data, validation data, and test data\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_name.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model.to(device),\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e0ffe3974c4464804a18e10c317020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0734, 'learning_rate': 1.774520856820744e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7641, 'learning_rate': 1.5490417136414883e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6954, 'learning_rate': 1.3235625704622324e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6354, 'learning_rate': 1.0980834272829764e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6174, 'learning_rate': 8.726042841037205e-06, 'epoch': 0.56}\n",
      "{'loss': 0.5836, 'learning_rate': 6.4712514092446456e-06, 'epoch': 0.68}\n",
      "{'loss': 0.5785, 'learning_rate': 4.216459977452086e-06, 'epoch': 0.79}\n",
      "{'loss': 0.584, 'learning_rate': 1.9616685456595265e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67611a84afe3498db9896b4c25e397f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(parent_dir+'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = MarianMTModel.from_pretrained(parent_dir+'model')\n",
    "\n",
    "# Translate a Chinese text\n",
    "input_text = \"金甲神兽\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
