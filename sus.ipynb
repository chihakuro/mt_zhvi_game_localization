{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of XLSX file paths (each file represents a separate dataset)\n",
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "csv_files = ['flang.csv', 'train.csv', 'test.csv']\n",
    "# Proportion for training data (e.g., 80%)\n",
    "train_proportion = 0.8\n",
    "\n",
    "# Initialize lists to store training and testing dataframes\n",
    "train_dataframes = []\n",
    "test_dataframes = []\n",
    "\n",
    "# Loop through each XLSX file\n",
    "for csv_file in csv_files:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, train_size=train_proportion)\n",
    "\n",
    "    # Append the training and testing DataFrames to the lists\n",
    "    train_dataframes.append(train_df)\n",
    "    test_dataframes.append(test_df)\n",
    "\n",
    "# Stack the training and testing DataFrames into one DataFrame with 2 columns\n",
    "train_df = pd.concat(train_dataframes, axis=0)\n",
    "test_df = pd.concat(test_dataframes, axis=0)\n",
    "\n",
    "# Save the training and testing DataFrames to CSV files\n",
    "train_df.to_csv(os.path.join(parent_dir, 'trainf.csv'), index=False, header=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(os.path.join(parent_dir, 'testf.csv'), index=False, header=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "fil = 'flang.xlsx'\n",
    "df = pd.read_excel(parent_dir + fil)\n",
    "df.to_csv(parent_dir + 'dataset.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = 'trainf.csv'\n",
    "df = pd.read_csv(parent_dir + fil, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'testf.csv', header=None, names=['text', 'ground_truth'])\n",
    "def html_to_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(html_to_text)\n",
    "# change type of ground_truth to string\n",
    "df['ground_truth'] = df['ground_truth'].astype(str)\n",
    "df['ground_truth'] = df['ground_truth'].apply(html_to_text)\n",
    "\n",
    "# save to csv (without)\n",
    "df.to_csv(parent_dir + 'test.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'dataseto.csv', header=None, names=['text', 'ground_truth'])\n",
    "\n",
    "# remove new line characters in each column\n",
    "df['text'] = df['text'].replace('\\n', ' ')\n",
    "df['ground_truth'] = df['ground_truth'].replace('\\n', ' ')\n",
    "\n",
    "# save to csv (without)\n",
    "df.to_csv(parent_dir + 'dataset.csv', index=False, header=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print a sample\n",
    "#extract content from dataframe\n",
    "one = df.iloc[6]\n",
    "print(one['text'])\n",
    "print(new_line_to_space('ç§\\næ¤'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent_dir + 'testf.csv', header=None, names=['text', 'ground_truth'])\n",
    "\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = df[df['text']== df['ground_truth']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(df,shit, indicator=True, how = 'outer').query('_merge==\"left_only\"').drop('_merge',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "57056-52974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv(parent_dir + 'testf.csv', index=False, header=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a text file and convert all to lowercase\n",
    "with open(parent_dir + 'raw_vietnamese.txt', 'w') as s:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "inp = parent_dir + 'raw_vietnamese.txt'\n",
    "model_prefix = 'spm_model'\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=inp, model_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = parent_dir + \"spm_model.model\"\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "\n",
    "text = \"TÃ­nh nÄƒng káº¿t hÃ´n vÃ´ cÃ¹ng phong phÃº bao gá»“m bá»“i dÆ°á»¡ng tÃ¬nh cáº£m, phá»¥ báº£n tÃ¬nh duyÃªn, táº·ng hoa, khÃ³a Ä‘á»“ng tÃ¢m, báº£o báº£oâ€¦giÃºp cho báº¡n vÃ  ná»­a kia ngÃ y cÃ ng tháº¯t cháº·t tÃ¬nh cáº£m - â€Äáº¥u Tháº§n Tuyá»‡t Tháº¿ - KhÃ´ng lo bá»‹ áº¿!\"\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "\n",
    "# Decode tokens back to text\n",
    "decoded_text = sp.decode_pieces(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Chinese text\n",
    "input_text = \"ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæœºå™¨ç¿»è¯‘çš„ç¤ºä¾‹ã€‚\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids, num_beams=4, max_length=150, no_repeat_ngram_size=3)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import MarianMTModel, MarianTokenizer\n",
    "# Start from here when restarting the notebook\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "import torch\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-vi\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Input: æƒ³è¦ç”¨æš—ç®­å·è¢­æˆ‘ï¼Ÿæ˜æ™ºçš„é€‰æ‹©ï¼Œå¯æƒœå¤±è´¥äº†ã€‚\n",
      "Vietnamese Translation: Má»™t sá»± lá»±a chá»n khÃ´n ngoan khi táº¥n cÃ´ng tÃ´i báº±ng mÅ©i tÃªn tá»‘i tÄƒm, nhÆ°ng khÃ´ng may lÃ  chÃºng ta Ä‘Ã£ tháº¥t báº¡i.\n"
     ]
    }
   ],
   "source": [
    "# Input Chinese text\n",
    "input_text = \"æƒ³è¦ç”¨æš—ç®­å·è¢­æˆ‘ï¼Ÿæ˜æ™ºçš„é€‰æ‹©ï¼Œå¯æƒœå¤±è´¥äº†ã€‚\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', 'target'],\n",
       "    num_rows: 70953\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train MarianMT on custom dataset \n",
    "'''from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parent_dir = 'D:/im_getting_a_raise/'\n",
    "\n",
    "df = pd.read_csv(parent_dir + 'dataset.csv', header=None, names=['text', 'ground_truth'], encoding='utf-8-sig')\n",
    "\n",
    "translation = df['text'].astype(str).tolist() #key\n",
    "target = df['ground_truth'].astype(str).tolist() #value\n",
    "\n",
    "# Initialize to a list of dictionaries in the format of {'translation': [translation1, translation2, ...], 'target': [target1, target2, ...]}\n",
    "data = {'translation': translation, 'target': target}\n",
    "\n",
    "# Convert the dictionary into a Dataset object\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets inside the Dataset object\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_val_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "train_val_dataset = train_val_dataset.train_test_split(test_size=0.25)\n",
    "train_dataset = train_val_dataset['train']\n",
    "val_dataset = train_val_dataset['test']\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>æ´»åŠ¨å°šæœªå¼€å¯ï¼Œæ•¬è¯·æœŸå¾…</td>\n",
       "      <td>Hoáº¡t Ä‘á»™ng chÆ°a má»Ÿ, vui lÃ²ng Ä‘á»£i!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>åˆ† äº«</td>\n",
       "      <td>Chia sáº»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å¤é“çƒ­è‚ </td>\n",
       "      <td>Nhiá»‡t tÃ¬nh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3æ˜Ÿ12é˜¶çº¢è‰²ä»™é•¯</td>\n",
       "      <td>TiÃªn Tráº¡c Äá» báº­c 12 3 Sao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>æˆ˜è½¦ç›´å‡10é˜¶ç¤¼åŒ…</td>\n",
       "      <td>QuÃ  Chiáº¿n Xa lÃªn giai 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e69ad690302b>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': 'ç„°å²­â‰¥3ä¸ª,åœ£è¾‰â‰¤1ä¸ª', 'target': 'Há»‡ Há»a â‰¥ 3, Há»‡ Kim â‰¤ 1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1198, 5292, 22, 809, 1943, 10313, 5228, 2429, 3, 693, 2791, 1117, 2455, 1176, 1943, 8401, 5228, 3, 2837, 1713, 911, 175, 1176, 1943, 8, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer(['æ‚¨è·å¾—äº†çœŸé¾™ç§˜å®ä»¤ï¼Œå¯è¿›å…¥è¿™é‡Œè¿›è¡Œé‡‘é¾™å¯»å®ï¼ŒæŠ½å–æ— ä¸Šé‡‘é¾™ï¼'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[5, 1, 22, 1, 3, 1, 3, 1, 8, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haimi\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"æ‚¨è·å¾—äº†çœŸé¾™ç§˜å®ä»¤ï¼Œå¯è¿›å…¥è¿™é‡Œè¿›è¡Œé‡‘é¾™å¯»å®ï¼ŒæŠ½å–æ— ä¸Šé‡‘é¾™ï¼\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 24827, 26521, 1, 656, 546, 3, 3073, 18478, 1, 1090, 546, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], [5, 2674, 519, 3, 620, 3341, 6899, 13696, 656, 546, 3341, 5952, 6037, 5971, 34241, 1090, 6213, 3902, 3, 1923, 3341, 720, 1516, 2763, 36287, 519, 3, 7211, 2797, 992, 1090, 36549, 15, 3902, 3, 3154, 2063, 3112, 8632, 1090, 2059, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[7518, 8558, 5, 1, 625, 3, 7518, 3882, 5, 1, 110, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], [1100, 149, 152, 625, 2731, 2672, 1123, 1222, 839, 956, 167, 918, 48, 364, 44718, 34241, 1090, 6213, 295, 459, 2, 4853, 839, 9, 1207, 2763, 1552, 1552, 3, 526, 30592, 5971, 295, 459, 2, 2207, 924, 8509, 1550, 2671, 110, 249, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "source_lang = \"translation\"\n",
    "target_lang = \"target\"\n",
    "data_list = [train_dataset, val_dataset, test_dataset]\n",
    "def preprocess_function(data_list):\n",
    "    inputs = data_list[\"translation\"]\n",
    "    targets = data_list[\"target\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "    # set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_function(train_dataset[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4319ef6e03204770aedc5a7a81860dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc2ddb4670a4cdfbccee70a2be1c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c34b49493b4802b07be5b29c2b33e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training data, validation data, and test data\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_name.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model.to(device),\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e0ffe3974c4464804a18e10c317020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0734, 'learning_rate': 1.774520856820744e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7641, 'learning_rate': 1.5490417136414883e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6954, 'learning_rate': 1.3235625704622324e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6354, 'learning_rate': 1.0980834272829764e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6174, 'learning_rate': 8.726042841037205e-06, 'epoch': 0.56}\n",
      "{'loss': 0.5836, 'learning_rate': 6.4712514092446456e-06, 'epoch': 0.68}\n",
      "{'loss': 0.5785, 'learning_rate': 4.216459977452086e-06, 'epoch': 0.79}\n",
      "{'loss': 0.584, 'learning_rate': 1.9616685456595265e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67611a84afe3498db9896b4c25e397f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(parent_dir+'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = MarianMTModel.from_pretrained(parent_dir+'model')\n",
    "\n",
    "# Translate a Chinese text\n",
    "input_text = \"é‡‘ç”²ç¥å…½\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Translate the text\n",
    "translated_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the translated IDs into text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Chinese Input:\", input_text)\n",
    "print(\"Vietnamese Translation:\", translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
